# CLAUDE.md - SmartSub API Backend

Documentation pour Claude Code sur l'architecture et le fonctionnement du backend FastAPI de Smart Subtitles.

## Vue d'ensemble

Le backend FastAPI traite les sous-titres en fusionnant intelligemment les pistes de langue cible (PT) et langue native (FR) selon le niveau de vocabulaire de l'utilisateur.

**Flow principal:**
```
Client HTTP ‚Üí FastAPI Endpoint ‚Üí Subtitle Fusion Engine ‚Üí OpenAI/DeepL Translation ‚Üí Processed Subtitles
```

## Architecture des fichiers

### Fichiers principaux

#### `main.py` (240+ lignes)
- **R√¥le:** Point d'entr√©e FastAPI avec s√©curit√© et rate limiting
- **Endpoints:**
  - `POST /fuse-subtitles` - Fusion de sous-titres avec traduction inline
  - `POST /proxy-railway` - Proxy pour Railway API
- **S√©curit√©:**
  - Rate limiting in-memory (10 req/min par IP)
  - Validation de taille de fichier (5MB max)
  - CORS restreint aux domaines Netflix
  - Gestion d'API keys serveur-side

#### `src/subtitle_fusion.py` (900+ lignes)
- **R√¥le:** Moteur principal de fusion de sous-titres
- **Classe:** `SubtitleFusionEngine`
- **M√©thode principale:** `fuse_subtitles(target_subs, native_subs, lang, native_lang, top_n, ...)`

**Algorithme de fusion:**
1. **Analyse vocabulaire** - D√©termine quels mots sont connus via frequency lists
2. **D√©cision par sous-titre:**
   - Tous les mots connus ‚Üí Garder sous-titre PT
   - 1 mot inconnu ‚Üí Traduction inline `mot (translation)`
   - 2+ mots inconnus ‚Üí Remplacer par sous-titre FR
3. **Synchronisation temporelle** - Bidirectionnelle avec calcul d'overlap
4. **Batch translation** - Collecte tous les mots √† traduire, puis traduction parall√®le

**Changements r√©cents (Janvier 2025):**
- ‚úÖ Suppression de la d√©duplication du cache pour √©viter la perte de sous-titres
- ‚úÖ Traduction contextuelle parfaite avec cl√©s uniques `word_INDEX`
- ‚úÖ Filtre "avalanche" - Compare FR avec PT pr√©c√©dent pour √©viter double affichage
- ‚ùå **BUG ACTUEL:** Syst√®me de cl√©s uniques cause √©chec de 33% des traductions (voir section Bugs)

#### `src/openai_translator.py` (390+ lignes)
- **R√¥le:** Traduction contextuelle via OpenAI GPT-4.1 Nano ou Google Gemini
- **Classe:** `OpenAITranslator`
- **M√©thodes cl√©s:**
  - `translate_batch_with_context()` - Traduction synchrone d'un batch
  - `translate_batch_parallel()` - Traduction parall√®le avec semaphore (8 requ√™tes simultan√©es)

**Architecture:**
- **Structured Outputs** - Schema Pydantic pour JSON garanti
- **Context-aware** - Chaque mot re√ßoit le texte du sous-titre comme contexte
- **Parallel execution** - Chunks de 18 mots, max 8 requ√™tes concurrentes
- **Cache in-memory** - √âvite les traductions r√©p√©t√©es

**Structured Output Schema:**
```python
class WordTranslation(BaseModel):
    word: str          # Le mot √† traduire
    translation: str   # La traduction
```

#### `src/frequency_loader.py`
- **R√¥le:** Chargement des listes de fr√©quence de mots au d√©marrage
- **Fonction:** `load_frequency_list(lang)` - Charge top 800 mots par langue
- **Langues support√©es:** EN, FR, PT, ES
- **Usage:** D√©termine si un mot est "connu" (dans top N)

#### `src/lemmatizer.py`
- **R√¥le:** Lemmatisation intelligente des mots
- **Fonction principale:** `smart_lemmatize_line(text, lang)`
- **Logique:**
  - Top 200 mots les plus fr√©quents ‚Üí **Pas de lemmatisation** (√©vite bugs comme "uma" ‚Üí "umar")
  - Autres mots ‚Üí Lemmatisation via `simplemma`

#### `src/srt_parser.py`
- **R√¥le:** Parsing et g√©n√©ration de fichiers SRT
- **Fonctions:**
  - `parse_srt(srt_content)` - Parse SRT en objets `Subtitle`
  - `generate_srt(subtitles)` - G√©n√®re SRT depuis objets
  - `normalize_words(text)` - Normalise texte (expand contractions, lowercase, filtre)

#### `src/deepl_api.py`
- **R√¥le:** Fallback translation sans contexte via DeepL
- **Usage:** Seulement si OpenAI √©choue
- **Limitation:** Pas de contexte subtitle, juste traduction de mot

## Data Flow d√©taill√©

### 1. Collection des mots √† traduire

**Code actuel (develop branch):**
```python
# subtitle_fusion.py lignes 526-629
subtitles_to_translate = []  # Liste de tuples (word, subtitle)

for current_target_sub in target_subs:
    # Analyse des mots inconnus
    if len(unknown_words) == 1:
        # Trouve le mot original via token mappings
        original_word = mapping.original_word  # Ex: "tens√£o]", "delegada", "motor."

        # Collecte le tuple (PAS de d√©duplication)
        subtitles_to_translate.append((original_word, current_target_sub))
```

**Probl√®me:** `original_word` contient la ponctuation attach√©e:
- `"tens√£o]"` (crochet du markup SRT `[m√∫sica de tens√£o]`)
- `"delegada"` (pas de ponctuation)
- `"motor."` (point de fin de phrase)

### 2. Construction des cl√©s uniques

**Code actuel (develop branch):**
```python
# subtitle_fusion.py lignes 829-838
word_contexts = {}
words_to_translate = []

for idx, (word, subtitle) in enumerate(subtitles_to_translate):
    unique_key = f"{word}_{idx}"  # Ex: "tens√£o]_126", "delegada_132"
    word_contexts[unique_key] = strip_html(subtitle.text)
    words_to_translate.append(unique_key)
```

**Envoy√© √† OpenAI:**
- `words_to_translate = ["tens√£o]_126", "motor._127", "delegada_132", ...]`
- `word_contexts = {"tens√£o]_126": "m√∫sica de tens√£o", "motor._127": "som do motor", ...}`

### 3. Traduction OpenAI

**Prompt g√©n√©r√©:**
```
TASK: Translate 18 Portuguese words to French.

WORD CONTEXTS (each word with its subtitle):
- "tens√£o]_126" appears in: "m√∫sica de tens√£o"
- "motor._127" appears in: "som do motor"
- "delegada_132" appears in: "Eu sou a delegada Lavelle."
...

TRANSLATION RULES:
1. Use the subtitle context to understand how the word is used
2. Provide natural translations as a native speaker would say
3. Keep translations concise (1-3 words maximum)
...
```

**Structured Output attendu:**
```json
{
  "translations": [
    {"word": "tens√£o]_126", "translation": "tension"},
    {"word": "motor._127", "translation": "moteur"},
    ...
  ]
}
```

**Probl√®me:** OpenAI peut "nettoyer" le champ `word`:
- Retourne `{"word": "tens√£o]", "translation": "tension"}` (sans `_126`)
- Ou `{"word": "tens√£o", "translation": "tension"}` (sans crochet ni index)
- Ou `{"word": "motor", "translation": "moteur"}` (sans point ni index)

### 4. Application des traductions

**Code actuel:**
```python
# subtitle_fusion.py lignes 882-908
for idx, (original_word, subtitle) in enumerate(subtitles_to_translate):
    unique_key = f"{original_word}_{idx}"  # Ex: "tens√£o]_126"
    translation = translations.get(unique_key)  # Cherche cl√© exacte

    if translation:
        # Applique la traduction ‚úÖ
        new_text = pattern.sub(f"{original_word} ({translation})", subtitle.text)
    else:
        # Pas de traduction trouv√©e ‚ùå
        logger.warning(f"‚ö†Ô∏è  No translation for '{original_word}' in subtitle {subtitle.index}")
```

**R√©sultat:** Si OpenAI retourne `"tens√£o]"` au lieu de `"tens√£o]_126"`:
- `translations = {"tens√£o]": "tension", ...}`
- `translations.get("tens√£o]_126")` ‚Üí `None`
- Warning: `"‚ö†Ô∏è  No translation for 'tens√£o]' in subtitle 1"`

## Bugs connus

### Bug #1: 33% des traductions √©chouent (ACTUEL - Janvier 2025)

**Sympt√¥me:** Logs Railway montrent `‚úÖ OpenAI parallel translation successful! Translated 232 subtitles` mais ensuite beaucoup de `‚ö†Ô∏è  No translation for 'tens√£o]' in subtitle 1 - keeping original`

**Statistiques:**
- Envoy√©: 348 mots (19 chunks √ó 18 + 1 chunk √ó 6)
- Retourn√©: 232 traductions
- **√âchou√©: 116 traductions (33%)**

**Exemples d'√©checs:**
```
‚úÖ Mots traduits: 'aumenta]' ‚Üí 'augmente' (subtitle 403)
‚úÖ Mots traduits: 'desconectadas!' ‚Üí 'd√©connect√©es' (subtitle 401)
‚ùå Pas de traduction: 'tens√£o]' (subtitle 1, 8, 36, 60, 77, ...)
‚ùå Pas de traduction: 'delegada' (subtitle 11)
‚ùå Pas de traduction: 'relat√≥rio,' (subtitle 132)
```

**Cause racine:**
1. **Cl√©s uniques avec index** - On envoie `"tens√£o]_126"` √† OpenAI
2. **OpenAI normalise** - Il retourne `{"word": "tens√£o]", "translation": "tension"}` (sans l'index!)
3. **Mismatch de cl√©s** - On cherche `translations["tens√£o]_126"]` ‚Üí `None`

**Pourquoi certains marchent et d'autres pas?**
OpenAI est **inconsistant** dans sa normalisation:
- Parfois il garde la cl√© exacte: `"aumenta]_403"` ‚Üí ‚úÖ
- Parfois il enl√®ve l'index: `"tens√£o]"` (au lieu de `"tens√£o]_126"`) ‚Üí ‚ùå
- Parfois il nettoie aussi la ponctuation: `"tens√£o"` (au lieu de `"tens√£o]_126"`) ‚Üí ‚ùå

**Code concern√©:**
- `subtitle_fusion.py:834-838` - Construction des cl√©s uniques
- `openai_translator.py:184-193` - Parsing de la r√©ponse OpenAI
- `subtitle_fusion.py:884-908` - Application des traductions avec `translations.get(unique_key)`

**Logs de debug ajout√©s (√† v√©rifier):**
```python
# openai_translator.py:186-188
logger.info(f"   [OPENAI] üîç DEBUG: OpenAI returned {len(parsed_data.translations)} translations")
logger.info(f"   [OPENAI] üîç DEBUG: First 5 translations: {[(item.word, item.translation) for item in parsed_data.translations[:5]]}")
```

**Ancien code (avant changements):**
```python
# Pas de cl√©s uniques, juste le mot avec ponctuation
if original_word not in unknown_words_to_translate:
    unknown_words_to_translate.append(original_word)  # Ex: "tens√£o]"

# Envoi √† OpenAI
word_contexts["tens√£o]"] = "m√∫sica de tens√£o"

# R√©ception
translations = {"tens√£o]": "tension"}  # OpenAI retourne la m√™me cl√©
translations.get("tens√£o]")  # ‚úÖ Match!
```

### Bug #2: Subtitle Loss (R√âSOLU - Janvier 2025)

**Sympt√¥me:** Sous-titres avec m√™me mot inconnu disparaissaient (seul le dernier √©tait gard√©)

**Cause:** D√©duplication via dict `word_to_subtitle_mapping[word] = subtitle` √©crasait les pr√©c√©dents

**Solution:** Suppression de la d√©duplication, utilisation de liste de tuples

### Bug #3: Double affichage PT+FR (R√âSOLU - Janvier 2025)

**Sympt√¥me:** PT 633 et FR 629+630 affich√©s simultan√©ment

**Cause:** FR 629 correspondait mieux √† PT 632, mais √©tait group√© avec PT 633

**Solution:** Filtre "avalanche" - Exclure FR qui overlap mieux avec PT pr√©c√©dent

## Fonctionnalit√©s cl√©s

### 1. Token Mapping System
- **R√¥le:** Pr√©serve l'alignement entre mots originaux et lemmatis√©s
- **Classe:** `TokenMapping` (dataclass)
- **Champs:** `original_word`, `normalized_word`, `lemmatized_word`, `is_filtered`, `original_index`
- **Usage:** R√©sout le probl√®me d'alignement quand certains mots sont filtr√©s

### 2. Smart Lemmatization
- **Top 200 mots:** Pas de lemmatisation (√©vite "uma" ‚Üí "umar")
- **Autres mots:** Lemmatisation via simplemma
- **Pourquoi:** Les outils NLP portugais ont des bugs sur les mots fonctionnels

### 3. Avalanche Prevention
- **Probl√®me:** Un sous-titre FR peut mieux matcher le PT pr√©c√©dent que le PT actuel
- **Solution:** Comparer overlap avec PT pr√©c√©dent, exclure si meilleur match
- **Code:** `subtitle_fusion.py:658-692`

### 4. Parallel Translation
- **Chunks:** 18 mots par requ√™te OpenAI
- **Concurrency:** 8 requ√™tes simultan√©es (38% du rate limit OpenAI de 500 RPM)
- **Performance:** 7.80s ‚Üí 3.95s (-49%), traitement total 10.03s ‚Üí 6.87s (-32%)

## Configuration

### Environment Variables
```bash
DEEPL_API_KEY=xxx          # DeepL API key (fallback)
OPENAI_API_KEY=xxx         # OpenAI API key (primary)
API_KEY=xxx                # Auth key pour endpoints
MAX_FILE_SIZE=5242880      # 5MB
```

### Startup
```bash
cd smartsub-api/
source venv/bin/activate
python main.py
```

### Testing
```bash
# Test complet de l'API
python test_fuse_subtitles_endpoint.py

# Tests unitaires
python -m pytest tests/ -v
```

## Performance

### M√©triques typiques (√©pisode 40 min)
- **Traduction:** ~350 mots, 20 chunks, 8.37s
- **Tokens:** ~12,000 input + ~4,000 output
- **Co√ªt:** ~$0.004 par √©pisode
- **Taux de remplacement:** ~72% des sous-titres trait√©s

## Notes importantes

1. **Ne jamais utiliser `npm run build` directement** - Toujours sp√©cifier staging ou production
2. **Railway auto-deploy** - Branch `develop` ‚Üí staging, branch `main` ‚Üí production
3. **Logs Railway** - Limite 500 logs/sec, risque de truncation si trop de debug
4. **Cache OpenAI** - In-memory, perdu au red√©marrage, pas de persistence
5. **Rate limiting** - In-memory, par IP, pas partag√© entre instances Railway

## TODO / Investigations en cours

- [ ] **PRIORIT√â 1:** R√©soudre le bug des 33% de traductions √©chou√©es
  - Option A: Faire matcher les cl√©s de mani√®re plus flexible (fuzzy matching)
  - Option B: Demander √† OpenAI de garder les cl√©s exactes dans le prompt
  - Option C: Post-processing pour mapper les r√©ponses d'OpenAI aux cl√©s originales

- [ ] V√©rifier si le probl√®me de ponctuation existe aussi (`.`, `,`, `!`, `?`)

- [ ] Am√©liorer le tokenization dans `create_alignment_mapping()` pour s√©parer ponctuation

- [ ] Analyser les logs Railway pour comprendre le pattern exact de normalisation d'OpenAI

## R√©f√©rences

- **MASTER_DOC.md** - Documentation compl√®te du projet (720+ lignes)
- **CLAUDE.md** (racine) - Documentation extension Chrome
- **Railway Production:** `smartsub-api-production.up.railway.app`
- **Railway Staging:** `smartsub-api-staging.up.railway.app`
